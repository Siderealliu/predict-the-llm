# Predict-the-LLM è¶…å‚æ•°ä¼˜åŒ–å®éªŒå®Œæ•´å®ç°æ–¹æ¡ˆ

## ğŸ“– é¡¹ç›®æ¦‚è¿°

### ç›®æ ‡
å›´ç»•H2O.aiçš„"é¢„æµ‹LLMè¾“å‡ºæ¥æº"ç«èµ›æ•°æ®ï¼Œè®¾è®¡ç³»ç»ŸåŒ–å®éªŒå¯¹æ¯”ä¸åŒè¶…å‚æ•°ä¼˜åŒ–(HPO)æ–¹æ³•ã€ç‰¹å¾å·¥ç¨‹ç­–ç•¥å’Œæ¨¡å‹æ¶æ„çš„æ•ˆæœã€‚

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
1. **Q1**: ä¸åŒHPOæ–¹æ³•åœ¨ç›¸åŒèµ„æºé¢„ç®—ä¸‹ï¼Œè°æ›´æœ‰æ•ˆï¼Ÿ
   - å¯¹æ¯”Grid Searchã€Random Searchã€Optuna TPEåœ¨å¤šåˆ†ç±»loglossä¸Šçš„è¡¨ç°ä¸æ”¶æ•›é€Ÿåº¦

2. **Q2**: ç‰¹å¾å’Œæ¨¡å‹çš„é€‰æ‹©å¯¹æœ€ç»ˆæ•ˆæœçš„å½±å“æœ‰å¤šå¤§ï¼Ÿ
   - å¯¹æ¯”TF-IDFç‰¹å¾ vs å¥å‘é‡ç‰¹å¾ï¼Œä»¥åŠçº¿æ€§æ¨¡å‹ vs æ ‘æ¨¡å‹

3. **Q3**: åœ¨è®­ç»ƒèµ„æºæœ‰é™çš„å‰æä¸‹ï¼Œæœ€åˆé€‚çš„ä¸€å¥—pipelineæ˜¯ä»€ä¹ˆï¼Ÿ
   - é€‰å‡ºã€Œæ€§èƒ½ä¸é”™ + è®­ç»ƒå¿« + å®ç°ç®€å•ã€çš„ç»„åˆ

### æ•°æ®æƒ…å†µ
- **è®­ç»ƒé›†**: 23,527æ¡æ ·æœ¬ (Question, Response, targetâˆˆ{0-6})
- **æµ‹è¯•é›†**: 6,008æ¡æ ·æœ¬
- **è¯„ä¼°æŒ‡æ ‡**: 7ç±»multiclass logloss
- **æäº¤æ ¼å¼**: æ¯ç±»æ¦‚ç‡çš„CSVæ–‡ä»¶

## ğŸ—ï¸ å®Œæ•´é¡¹ç›®ç›®å½•ç»“æ„

```
predict-the-llm/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py
â”œâ”€â”€ Outline.md                    # æœ¬æ–‡ä»¶ï¼šå®Œæ•´å®ç°æ–¹æ¡ˆ
â”œâ”€â”€ data/
â”‚   â””â”€â”€ h2oai-predict-the-llm/
â”‚       â”œâ”€â”€ train.csv (23,527æ¡)
â”‚       â”œâ”€â”€ test.csv (6,008æ¡)
â”‚       â””â”€â”€ sample_submission.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_config.py        # æ•°æ®è·¯å¾„ã€åˆ’åˆ†ç­–ç•¥é…ç½®
â”‚   â”‚   â”œâ”€â”€ model_config.py       # è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰
â”‚   â”‚   â””â”€â”€ experiment_config.py  # 5ä¸ªå®éªŒç»„çš„å…·ä½“é…ç½®
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py        # CSVæ•°æ®åŠ è½½ä¸åŸºç¡€æ¸…æ´—
â”‚   â”‚   â”œâ”€â”€ splitter.py           # GroupKFoldä¸train/valåˆ’åˆ†
â”‚   â”‚   â””â”€â”€ preprocessor.py       # æ–‡æœ¬é¢„å¤„ç†(æ‹¼æ¥ã€æ¸…æ´—)
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # ç‰¹å¾æå–å™¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ tfidf_features.py     # TF-IDFç‰¹å¾(word+charçº§åˆ«)
â”‚   â”‚   â”œâ”€â”€ embedding_features.py # å¥å‘é‡ç‰¹å¾(MiniLMé¢„è®¡ç®—)
â”‚   â”‚   â””â”€â”€ statistical_features.py # ç»Ÿè®¡ç‰¹å¾(é•¿åº¦ã€æ¯”ä¾‹ç­‰)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py        # æ¨¡å‹åŸºç±»
â”‚   â”‚   â”œâ”€â”€ logistic_regression.py # LRå°è£…
â”‚   â”‚   â””â”€â”€ lightgbm_model.py    # LightGBMå°è£…
â”‚   â”œâ”€â”€ hpo/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_hpo.py          # HPOåŸºç±»
â”‚   â”‚   â”œâ”€â”€ grid_search.py       # GridSearchCVå®ç°
â”‚   â”‚   â”œâ”€â”€ random_search.py     # RandomizedSearchCVå®ç°
â”‚   â”‚   â””â”€â”€ optuna_tpe.py        # Optuna TPEå®ç°
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py           # è¯„ä¼°æŒ‡æ ‡(loglossç­‰)
â”‚   â”‚   â””â”€â”€ validator.py         # äº¤å‰éªŒè¯å™¨
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ group_a_baseline.py  # ç»„A: BaselineåŸºçº¿å®éªŒ
â”‚   â”‚   â”œâ”€â”€ group_b_tfidf_lr.py  # ç»„B: TF-IDF+LRä¸‰ç§HPOå¯¹æ¯”
â”‚   â”‚   â”œâ”€â”€ group_c_embedding_lgb.py # ç»„C: Embedding+LightGBMä¸‰ç§HPO
â”‚   â”‚   â”œâ”€â”€ group_d_feature_comparison.py # ç»„D: å››ç§ç‰¹å¾+æ¨¡å‹ç»„åˆå¯¹æ¯”
â”‚   â”‚   â””â”€â”€ group_e_ablation.py  # ç»„E: æ¶ˆèå®éªŒ(Q vs A, ç»Ÿè®¡ç‰¹å¾ç­‰)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py            # ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ io_utils.py          # ç»“æœä¿å­˜/åŠ è½½(JSON, CSV)
â”‚   â”‚   â””â”€â”€ visualization.py     # æ”¶æ•›æ›²çº¿ã€å¯¹æ¯”å›¾å¯è§†åŒ–
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_pipeline.py     # PipelineåŸºç±»
â”‚       â”œâ”€â”€ tfidf_pipeline.py    # TF-IDF+æ¨¡å‹Pipeline
â”‚       â””â”€â”€ embedding_pipeline.py # Embedding+æ¨¡å‹Pipeline
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ experiments/             # å®éªŒç»„ç»“æœå­˜å‚¨
â”‚   â”‚   â”œâ”€â”€ group_a/
â”‚   â”‚   â”œâ”€â”€ group_b/
â”‚   â”‚   â”œâ”€â”€ group_c/
â”‚   â”‚   â”œâ”€â”€ group_d/
â”‚   â”‚   â””â”€â”€ group_e/
â”‚   â”œâ”€â”€ checkpoints/            # è®­ç»ƒè¿‡ç¨‹æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ cache/                  # Embeddingç­‰ä¸­é—´ç»“æœç¼“å­˜
â”‚   â”‚   â””â”€â”€ embeddings/
â”‚   â”‚       â””â”€â”€ train_embeddings.npy
â”‚   â””â”€â”€ final/                  # æœ€ç»ˆç»“æœ
â”‚       â”œâ”€â”€ best_model.pkl      # æœ€ä½³æ¨¡å‹
â”‚       â”œâ”€â”€ submission.csv      # Kaggleæäº¤æ–‡ä»¶
â”‚       â””â”€â”€ experiment_summary.csv # æ‰€æœ‰å®éªŒå¯¹æ¯”è¡¨
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ experiment.log          # ä¸»å®éªŒæ—¥å¿—
â”‚   â””â”€â”€ hpo_*.log               # å„HPOæ–¹æ³•è¯¦ç»†æ—¥å¿—
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_all_experiments.py  # æ‰¹é‡è¿è¡Œæ‰€æœ‰å®éªŒ
â”‚   â”œâ”€â”€ run_single_experiment.py # è¿è¡Œå•ä¸ªå®éªŒ
â”‚   â””â”€â”€ prepare_submission.py   # ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶
â””â”€â”€ venv/                       # è™šæ‹Ÿç¯å¢ƒ
```

## ğŸ“Š 5ä¸ªå®éªŒç»„è¯¦ç»†è®¾è®¡æ–¹æ¡ˆ

### å®éªŒç»„A: Baselineä¸æ•°æ®sanity check
**ç›®æ ‡**: å»ºç«‹åŸºçº¿ï¼Œç¡®è®¤pipelineæ­£å¸¸ï¼ŒéªŒè¯GroupKFoldçš„é‡è¦æ€§

#### A1: åŸºç¡€TF-IDF+LR
- **ç‰¹å¾**: TF-IDF(word-level, ngram_range=(1,2), max_features=40000)
- **æ¨¡å‹**: LogisticRegression(C=1.0, class_weight=None)
- **æµç¨‹**: ç›´æ¥è®­ç»ƒï¼Œæ— HPO
- **è®°å½•**: CV loglossã€holdout loglossã€Kaggle logloss
- **æ„ä¹‰**: éªŒè¯æœ€ç®€å•çš„pipelineèƒ½å¦æ­£å¸¸å·¥ä½œ

#### A2: å¢å¼ºTF-IDF+LR
- **ç‰¹å¾**: TF-IDF(word+charç»„åˆ, ä½¿ç”¨FeatureUnion)
- **æ¨¡å‹**: LR(C=0.1, class_weight='balanced')
- **æ„ä¹‰**: ç¨ä½œè°ƒå‚ï¼ŒéªŒè¯ç»„åˆç‰¹å¾çš„æ•ˆæœ

#### æ•°æ®æ³„éœ²éªŒè¯(å¯é€‰)
- **å¯¹æ¯”**: GroupKFold vs StratifiedKFold
- **é¢„æœŸ**: StratifiedKFoldä¼šæœ‰è™šé«˜çš„CVåˆ†æ•°ï¼ˆå› ä¸ºåŒä¸€questionå¯èƒ½åœ¨ä¸åŒfoldï¼‰
- **æ„ä¹‰**: å¼ºè°ƒGroupKFoldçš„å¿…è¦æ€§

### å®éªŒç»„B: å›ºå®šTF-IDF+LRï¼Œä¸‰ç§HPOæ–¹æ³•å¯¹æ¯”
**ç›®æ ‡**: åœ¨ç®€å•åœºæ™¯ä¸‹å¯¹æ¯”ä¸‰ç§HPOæ–¹æ³•çš„æ•ˆç‡ä¸æ•ˆæœ

#### å›ºå®šé…ç½®
- **ç‰¹å¾**: TF-IDF(word+charç»„åˆ, é»˜è®¤å‚æ•°)
- **æ¨¡å‹**: LogisticRegression
- **æœç´¢ç©ºé—´**: Cã€ngram_rangeã€max_featuresã€class_weight

#### B1: GridSearchCV
```python
param_grid = {
    'tfidf__ngram_range': [(1,1), (1,2)],
    'tfidf__max_features': [20000, 40000],
    'classifier__C': [0.01, 0.1, 1.0, 10.0],
    'classifier__class_weight': [None, 'balanced']
}
```
- **è¯•éªŒæ•°**: 2Ã—2Ã—4Ã—2 = 32æ¬¡
- **é¢„æœŸ**: ç©·ä¸¾æœç´¢ï¼Œä½†è®¡ç®—é‡å¤§

#### B2: RandomizedSearchCV
```python
param_dist = {
    'tfidf__ngram_range': [(1,1), (1,2), (1,3)],
    'tfidf__max_features': [20000, 40000, 60000],
    'classifier__C': loguniform(1e-3, 1e2),
    'classifier__class_weight': [None, 'balanced']
}
```
- **è¯•éªŒæ•°**: 30-40æ¬¡ï¼ˆä¸Gridç›¸è¿‘ï¼‰
- **é¢„æœŸ**: åœ¨ç›¸åŒé¢„ç®—ä¸‹è¦†ç›–æ›´å¤šå‚æ•°ç©ºé—´

#### B3: Optuna TPE
```python
def objective(trial):
    ngram_range = trial.suggest_categorical('ngram_range', [(1,1), (1,2), (1,3)])
    max_features = trial.suggest_int('max_features', 20000, 60000, step=20000)
    C = trial.suggest_float('C', 1e-3, 1e2, log=True)
    class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])
    # ...æ„å»ºpipelineå¹¶è¯„ä¼°
    return cv_score
```
- **è¯•éªŒæ•°**: 40æ¬¡
- **é¢„æœŸ**: ç›¸æ¯”éšæœºæœç´¢ï¼ŒTPEä¼šæ›´å¿«æ”¶æ•›åˆ°ä¼˜è§£

#### å¯¹æ¯”ç»´åº¦
1. **æ€§èƒ½**: best CV logloss
2. **æ•ˆç‡**: å¹³å‡æ¯trialæ—¶é—´
3. **æ”¶æ•›**: trial_index vs best_so_faræ›²çº¿
4. **ç¨³å®šæ€§**: ä¸åŒéšæœºç§å­ä¸‹ç»“æœæ–¹å·®

### å®éªŒç»„C: å›ºå®šSentence Embedding+LightGBMï¼Œä¸‰ç§HPOå¯¹æ¯”
**ç›®æ ‡**: åœ¨å¤æ‚åœºæ™¯ä¸‹éªŒè¯HPOæ–¹æ³•çš„æ‰©å±•æ€§

#### å›ºå®šé…ç½®
- **ç‰¹å¾**: Sentence Embedding(all-MiniLM-L6-v2, 384ç»´)
- **æ¨¡å‹**: LightGBMå¤šåˆ†ç±»
- **é¢„å¤„ç†**: ç¦»çº¿é¢„è®¡ç®—embeddingå¹¶ç¼“å­˜

#### åµŒå…¥é¢„è®¡ç®—
```python
# data/embedding_features.py
class EmbeddingPreprocessor:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.cache_path = 'results/cache/embeddings/'

    def compute_and_cache(self, texts):
        embeddings = self.model.encode(texts, show_progress_bar=True)
        np.save(os.path.join(self.cache_path, 'train_embeddings.npy'), embeddings)
        return embeddings

    def load_cache(self):
        return np.load(os.path.join(self.cache_path, 'train_embeddings.npy'))
```

#### C1: GridSearchCV
```python
param_grid = {
    'num_leaves': [31, 63, 127],
    'max_depth': [5, 7, -1],
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [200, 500]
}
```
- **è¯•éªŒæ•°**: 3Ã—3Ã—3Ã—2 = 54æ¬¡
- **é—®é¢˜**: é«˜ç»´ç©ºé—´ä¸‹ç½‘æ ¼æœç´¢æ•ˆç‡æä½

#### C2: RandomizedSearchCV
```python
param_dist = {
    'num_leaves': randint(16, 255),
    'max_depth': [-1, 4, 6, 8, 10],
    'learning_rate': loguniform(1e-3, 0.3),
    'n_estimators': randint(100, 1000),
    'min_data_in_leaf': randint(10, 200),
    'feature_fraction': uniform(0.6, 0.4),
    'lambda_l1': loguniform(1e-4, 10),
    'lambda_l2': loguniform(1e-4, 10)
}
```
- **è¯•éªŒæ•°**: 30-50æ¬¡
- **é¢„æœŸ**: ç›¸æ¯”Gridæ›´é«˜æ•ˆ

#### C3: Optuna TPE
```python
def objective(trial):
    num_leaves = trial.suggest_int('num_leaves', 16, 255)
    max_depth = trial.suggest_categorical('max_depth', [-1, 4, 6, 8, 10])
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
    # ...æ›´å¤šå‚æ•°
    # ä½¿ç”¨æ—©æœŸåœæ­¢å‡å°‘è®­ç»ƒæ—¶é—´
    params = {
        'objective': 'multiclass',
        'num_class': 7,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'verbose': -1
    }
    # CVè¯„ä¼°...
    return cv_score
```
- **è¯•éªŒæ•°**: 60æ¬¡ï¼ˆç»™TPEæ›´å¤šç©ºé—´å±•ç°ä¼˜åŠ¿ï¼‰
- **é¢„æœŸ**: TPEåœ¨é«˜ç»´ç©ºé—´ä¼˜åŠ¿æ›´æ˜æ˜¾

#### åˆ†æé‡ç‚¹
1. **è®¡ç®—å¤æ‚åº¦**: LightGBMè®­ç»ƒæ—¶é—´ vs LR
2. **æœç´¢æ•ˆç‡**: TPEåœ¨å¤æ‚ç©ºé—´çš„ä¼˜åŠ¿
3. **å‰ªææ•ˆæœ**: MedianPrunerå‡å°‘æ— æ•ˆtrial

### å®éªŒç»„D: ç»Ÿä¸€Optunaå¯¹æ¯”å››ç§ç‰¹å¾+æ¨¡å‹ç»„åˆ
**ç›®æ ‡**: å…¬å¹³å¯¹æ¯”ä¸åŒç‰¹å¾è¡¨ç¤ºå’Œæ¨¡å‹çš„ç»„åˆ

#### ç»Ÿä¸€è®¾ç½®
- **HPOæ–¹æ³•**: Optuna TPE (n_trials=50)
- **CVç­–ç•¥**: GroupKFold(n_splits=5)
- **æŒ‡æ ‡**: åŒæ ·çš„loglossè¯„ä¼°

#### å››ç§ç»„åˆ

**D1: TF-IDF(word only) + LR**
```python
feature_space = {
    'type': 'fixed',  # ç‰¹å¾ç±»å‹å›ºå®šä¸ºTF-IDF word
    'ngram_range': trial.suggest_categorical('ngram_range', [(1,1), (1,2), (1,3)]),
    'max_features': trial.suggest_int('max_features', 20000, 60000, step=20000)
}
model_space = {
    'C': trial.suggest_float('C', 1e-3, 1e2, log=True),
    'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])
}
```

**D2: TF-IDF(word+char) + LR**
```python
feature_space = {
    'type': 'tfidf_union',
    'word_ngram': trial.suggest_categorical('word_ngram', [(1,1), (1,2), (1,3)]),
    'char_ngram': trial.suggest_categorical('char_ngram', [(3,5), (3,6)]),
    'max_features': trial.suggest_int('max_features', 20000, 60000, step=20000)
}
```

**D3: Sentence Embedding + LR**
```python
feature_space = {
    'type': 'embedding',
    'model_name': 'all-MiniLM-L6-v2',  # å›ºå®š
    'dimension': 384  # å›ºå®š
}
model_space = {
    'C': trial.suggest_float('C', 1e-3, 1e2, log=True),
    'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])
}
```

**D4: Sentence Embedding + LightGBM**
```python
feature_space = {
    'type': 'embedding',
    'model_name': 'all-MiniLM-L6-v2'
}
model_space = {
    'num_leaves': trial.suggest_int('num_leaves', 16, 255),
    'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
    # ...æ›´å¤šLightGBMå‚æ•°
}
```

#### å¯¹æ¯”ç»´åº¦
1. **æ€§èƒ½**: D1-D4çš„best CV loglossæ’åº
2. **æ•ˆç‡**: å¹³å‡æ¯trialæ—¶é—´
3. **ç‰¹å¾ç»´åº¦**: ç¨€ç–é«˜ç»´ vs å¯†é›†ä½ç»´
4. **è®­ç»ƒé€Ÿåº¦**: LR vs LightGBM
5. **å†…å­˜å ç”¨**: ä¸åŒç‰¹å¾çš„å†…å­˜éœ€æ±‚

#### é¢„æœŸç»“è®º
- **D1 vs D2**: char-level TF-IDFæ˜¯å¦å¸¦æ¥æå‡
- **D2 vs D3**: ç¨€ç–è¯è¢‹ vs å¯†é›†è¯­ä¹‰å‘é‡
- **D3 vs D4**: çº¿æ€§ vs æ ‘æ¨¡å‹åœ¨embeddingä¸Šçš„è¡¨ç°

### å®éªŒç»„E: æ¶ˆèå®éªŒä¸æ·±åº¦åˆ†æ
**ç›®æ ‡**: æ·±å…¥ç†è§£æ•°æ®ç‰¹æ€§ï¼Œè¯†åˆ«å…³é”®ç‰¹å¾

#### E1-E3: Q vs Aé‡è¦æ€§åˆ†æ
**é—®é¢˜**: LLMé£æ ¼ä¸»è¦ä½“ç°åœ¨questionè¿˜æ˜¯answerä¸­ï¼Ÿ

```python
# E1: ä»…ç”¨questionæ–‡æœ¬
text = '[Q] ' + df['Question']

# E2: ä»…ç”¨answeræ–‡æœ¬
text = '[A] ' + df['Response']

# E3: question+answeræ‹¼æ¥(æ ‡å‡†åšæ³•)
text = '[Q] ' + df['Question'] + ' [A] ' + df['Response']
```
- **ç»Ÿä¸€é…ç½®**: TF-IDF+LR, Optuna (n_trials=30)
- **åˆ†æ**: æ¯”è¾ƒä¸‰ç§è®¾ç½®çš„best CV logloss
- **æ„ä¹‰**: äº†è§£å“ªéƒ¨åˆ†æ–‡æœ¬æ›´æœ‰åˆ¤åˆ«æ€§

#### E4: ç»Ÿè®¡ç‰¹å¾ + LightGBM
**é—®é¢˜**: ç®€å•çš„ç»Ÿè®¡ç‰¹å¾èƒ½å¦æ•æ‰LLMé£æ ¼å·®å¼‚ï¼Ÿ

```python
class StatisticalFeatures:
    def extract(self, text):
        features = {
            'char_count': len(text),
            'word_count': len(text.split()),
            'avg_word_len': np.mean([len(w) for w in text.split()]),
            'punct_ratio': sum(c in '.,!?;:' for c in text) / len(text),
            'upper_ratio': sum(c.isupper() for c in text) / len(text),
            'digit_ratio': sum(c.isdigit() for c in text) / len(text),
            'sentence_count': text.count('.') + text.count('!') + text.count('?')
        }
        return np.array(list(features.values()))
```
- **æ¨¡å‹**: LightGBM (é€‚åˆæ•°å€¼ç‰¹å¾)
- **åˆ†æ**: ç»Ÿè®¡ç‰¹å¾èƒ½åˆ°å¤šå°‘logloss
- **æ„ä¹‰**: è¯„ä¼°"é£æ ¼"ç‰¹å¾çš„å¯è§£é‡Šæ€§

#### E5: ç»Ÿè®¡ç‰¹å¾ + TF-IDFæ‹¼æ¥
**é—®é¢˜**: ç»Ÿè®¡ç‰¹å¾èƒ½å¦è¡¥å……TF-IDFï¼Ÿ

```python
# ColumnTransformerç»„åˆ
pipeline = Pipeline([
    ('features', ColumnTransformer([
        ('tfidf', TfidfVectorizer(...), 0),  # æ–‡æœ¬ç‰¹å¾
        ('stat', StatisticalFeatures(), 1)   # ç»Ÿè®¡ç‰¹å¾
    ])),
    ('classifier', LogisticRegression(...))
])
```
- **åˆ†æ**: æ‹¼æ¥ålogloss vs çº¯TF-IDFçš„æå‡
- **æ„ä¹‰**: ç»Ÿè®¡ç‰¹å¾çš„å¢é‡ä»·å€¼

#### E6: é”™è¯¯åˆ†æ(å¯é€‰æ·±åº¦)
**é—®é¢˜**: å“ªäº›LLMå®¹æ˜“è¢«æ··æ·†ï¼Ÿ

```python
# æ··æ·†çŸ©é˜µåˆ†æ
from sklearn.metrics import confusion_matrix

def analyze_confusion(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.savefig('results/confusion_matrix.png')

    # è¯†åˆ«æœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹
    np.fill_diagonal(cm, 0)  # å»æ‰å¯¹è§’çº¿
    confusion_pairs = []
    for i in range(7):
        for j in range(7):
            if cm[i, j] > 0:
                confusion_pairs.append((class_names[i], class_names[j], cm[i, j]))
    confusion_pairs.sort(key=lambda x: x[2], reverse=True)
    return confusion_pairs[:10]
```

**å…¸å‹é”™è¯¯æ¡ˆä¾‹åˆ†æ**:
- é€‰æ‹©å‡ ä¸ªè¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
- åˆ†æå…¶question/responseç‰¹ç‚¹
- è®¨è®ºä¸ºä»€ä¹ˆä¼šæ··æ·†

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°ç»†èŠ‚

### 1. æ•°æ®åˆ’åˆ†ç­–ç•¥

#### GroupKFoldå®ç°
```python
# data/splitter.py
class GroupKFoldSplitter:
    def __init__(self, n_splits=5, group_col='question_hash', test_size=0.2):
        self.n_splits = n_splits
        self.group_col = group_col
        self.test_size = test_size

    def split(self, df):
        # åˆ›å»ºquestion hashä½œä¸ºgroup
        df['question_hash'] = df['Question'].apply(
            lambda x: abs(hash(x)) % 10000  # 0-9999çš„hash bucket
        )

        # 80/20åˆ’åˆ†
        train_df, holdout_df = train_test_split(
            df, test_size=self.test_size,
            stratify=df['target'],  # ä¿æŒlabelåˆ†å¸ƒ
            random_state=42
        )

        # è®­ç»ƒé›†ä¸Šåš5æŠ˜GroupKFold
        gkf = GroupKFold(n_splits=self.n_splits)
        for train_idx, val_idx in gkf.split(
            train_df, train_df['target'], train_df['question_hash']
        ):
            yield train_idx, val_idx

        # holdoutç”¨äºæœ€ç»ˆéªŒè¯
        yield None, holdout_df.index  # æ ‡è®°è¿™æ˜¯holdout
```

#### ä½¿ç”¨ç¤ºä¾‹
```python
splitter = GroupKFoldSplitter()
for fold, (train_idx, val_idx) in enumerate(splitter.split(df)):
    if train_idx is None:  # holdout
        X_holdout = df.loc[val_idx, ['Question', 'Response']]
        y_holdout = df.loc[val_idx, 'target']
    else:
        X_train = df.loc[train_idx, ['Question', 'Response']]
        y_train = df.loc[train_idx, 'target']
        X_val = df.loc[val_idx, ['Question', 'Response']]
        y_val = df.loc[val_idx, 'target']
```

### 2. æ–‡æœ¬é¢„å¤„ç†

#### ç»Ÿä¸€é¢„å¤„ç†Pipeline
```python
# data/preprocessor.py
class TextPreprocessor:
    def __init__(self, lowercase=True, concatenate=True):
        self.lowercase = lowercase
        self.concatenate = concatenate

    def fit_transform(self, df):
        texts = []
        for _, row in df.iterrows():
            question = str(row['Question'])
            answer = str(row['Response'])

            if self.concatenate:
                text = f"[Q] {question} [A] {answer}"
            else:
                text = question + " " + answer

            if self.lowercase:
                text = text.lower()

            # å»é™¤å¤šä½™ç©ºç™½ä½†ä¿ç•™æ ‡ç‚¹
            text = re.sub(r'\s+', ' ', text).strip()
            texts.append(text)

        return texts

    def transform(self, df):
        return self.fit_transform(df)
```

### 3. ç‰¹å¾å·¥ç¨‹å®ç°

#### TF-IDFç»„åˆç‰¹å¾
```python
# features/tfidf_features.py
class TfidfFeatureExtractor:
    def __init__(self, word_params, char_params, use_feature_union=True):
        self.word_params = word_params
        self.char_params = char_params
        self.use_feature_union = use_feature_union

    def build_pipeline(self):
        if self.use_feature_union:
            pipeline = Pipeline([
                ('preprocessor', TextPreprocessor()),
                ('features', FeatureUnion([
                    ('word_tfidf', TfidfVectorizer(**self.word_params)),
                    ('char_tfidf', TfidfVectorizer(**self.char_params))
                ])),
                ('classifier', LogisticRegression())
            ])
        else:
            pipeline = Pipeline([
                ('preprocessor', TextPreprocessor()),
                ('tfidf', TfidfVectorizer(**self.word_params)),
                ('classifier', LogisticRegression())
            ])
        return pipeline
```

#### å¥å‘é‡ç‰¹å¾
```python
# features/embedding_features.py
class EmbeddingFeatureExtractor:
    def __init__(self, model_name='all-MiniLM-L6-v2', cache_dir='results/cache/embeddings'):
        self.model_name = model_name
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def compute_embeddings(self, texts, force_recompute=False):
        cache_file = os.path.join(self.cache_dir, f'{self.model_name.replace("/", "_")}.npy')

        if not force_recompute and os.path.exists(cache_file):
            print(f"Loading cached embeddings from {cache_file}")
            return np.load(cache_file)

        print(f"Computing embeddings with {self.model_name}...")
        model = SentenceTransformer(self.model_name)
        embeddings = model.encode(
            texts,
            show_progress_bar=True,
            batch_size=64,
            convert_to_numpy=True
        )

        np.save(cache_file, embeddings)
        print(f"Embeddings saved to {cache_file}")
        return embeddings

    def build_pipeline(self, lgb_params):
        # å…ˆé¢„è®¡ç®—embedding
        # ç„¶åæ„å»ºè½»é‡pipeline
        pipeline = Pipeline([
            ('classifier', lgb.LGBMClassifier(**lgb_params))
        ])
        return pipeline
```

### 4. æ¨¡å‹å°è£…

#### ç»Ÿä¸€æ¨¡å‹æ¥å£
```python
# models/base_model.py
class BaseModel:
    def __init__(self, model_type, params):
        self.model_type = model_type
        self.params = params
        self.model = self._build_model()
        self.is_fitted = False

    def _build_model(self):
        if self.model_type == 'logistic_regression':
            return LogisticRegression(
                C=self.params.get('C', 1.0),
                class_weight=self.params.get('class_weight', None),
                max_iter=self.params.get('max_iter', 500),
                solver='liblinear' if self.params.get('penalty') == 'l2' else 'saga',
                penalty=self.params.get('penalty', 'l2'),
                random_state=42,
                n_jobs=-1
            )
        elif self.model_type == 'lightgbm':
            return lgb.LGBMClassifier(
                objective='multiclass',
                num_class=7,
                metric='multi_logloss',
                boosting_type='gbdt',
                num_leaves=self.params.get('num_leaves', 31),
                max_depth=self.params.get('max_depth', -1),
                learning_rate=self.params.get('learning_rate', 0.1),
                n_estimators=self.params.get('n_estimators', 500),
                min_data_in_leaf=self.params.get('min_data_in_leaf', 20),
                feature_fraction=self.params.get('feature_fraction', 0.8),
                bagging_fraction=self.params.get('bagging_fraction', 0.8),
                lambda_l1=self.params.get('lambda_l1', 0),
                lambda_l2=self.params.get('lambda_l2', 0),
                verbose=-1,
                random_state=42,
                n_jobs=-1,
                force_col_wise=True
            )
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")

    def fit(self, X, y, **kwargs):
        self.model.fit(X, y, **kwargs)
        self.is_fitted = True
        return self

    def predict_proba(self, X):
        if not self.is_fitted:
            raise RuntimeError("Model must be fitted before prediction")
        return self.model.predict_proba(X)

    def predict(self, X):
        if not self.is_fitted:
            raise RuntimeError("Model must be fitted before prediction")
        return self.model.predict(X)
```

### 5. HPOæ–¹æ³•å®ç°

#### Grid Searchå°è£…
```python
# hpo/grid_search.py
class GridSearchHPO:
    def __init__(self, pipeline, param_grid, cv_strategy, scoring='neg_log_loss'):
        self.pipeline = pipeline
        self.param_grid = param_grid
        self.cv_strategy = cv_strategy
        self.scoring = scoring

    def optimize(self, n_jobs=-1):
        print(f"Starting Grid Search with {len(ParameterGrid(self.param_grid))} combinations...")

        start_time = time.time()
        search = GridSearchCV(
            estimator=self.pipeline,
            param_grid=self.param_grid,
            cv=self.cv_strategy,
            scoring=self.scoring,
            n_jobs=n_jobs,
            verbose=1,
            refit=True
        )
        search.fit(self.X_text, self.y)
        elapsed_time = time.time() - start_time

        results = {
            'method': 'grid_search',
            'n_trials': len(ParameterGrid(self.param_grid)),
            'total_time_seconds': elapsed_time,
            'best_params': search.best_params_,
            'best_cv_score': search.best_score_,
            'all_results': search.cv_results_,
            'refit_time': search.refit_time_
        }

        return results
```

#### Optuna TPEå°è£…
```python
# hpo/optuna_tpe.py
class OptunaTPEHPO:
    def __init__(self, pipeline, param_space, cv_strategy, scoring='neg_log_loss'):
        self.pipeline = pipeline
        self.param_space = param_space
        self.cv_strategy = cv_strategy
        self.scoring = scoring
        self.trial_history = []

    def _sample_params(self, trial):
        params = {}
        for param_name, param_config in self.param_space.items():
            if param_config['type'] == 'float':
                if param_config.get('log', False):
                    params[param_name] = trial.suggest_float(
                        param_name,
                        param_config['low'],
                        param_config['high'],
                        log=True
                    )
                else:
                    params[param_name] = trial.suggest_float(
                        param_name,
                        param_config['low'],
                        param_config['high']
                    )
            elif param_config['type'] == 'int':
                params[param_name] = trial.suggest_int(
                    param_name,
                    param_config['low'],
                    param_config['high'],
                    step=param_config.get('step', 1)
                )
            elif param_config['type'] == 'categorical':
                params[param_name] = trial.suggest_categorical(
                    param_name,
                    param_config['choices']
                )
            else:
                raise ValueError(f"Unsupported parameter type: {param_config['type']}")
        return params

    def _build_pipeline(self, params):
        # æ ¹æ®paramsåŠ¨æ€æ„å»ºpipeline
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„param_spaceç»“æ„è°ƒæ•´
        return self.pipeline  # ç®€åŒ–ç¤ºæ„

    def optimize(self, n_trials, n_jobs=1, direction='maximize'):
        print(f"Starting Optuna TPE with {n_trials} trials...")

        study = optuna.create_study(
            direction='maximize',
            sampler=optuna.samplers.TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
        )

        def objective(trial):
            # é‡‡æ ·å‚æ•°
            params = self._sample_params(trial)

            # æ„å»ºpipeline
            pipeline = self._build_pipeline(params)

            # CVè¯„ä¼°
            start_time = time.time()
            scores = cross_val_score(
                pipeline, self.X_text, self.y,
                cv=self.cv_strategy,
                scoring=self.scoring,
                n_jobs=n_jobs
            )
            elapsed_time = time.time() - start_time

            # è®°å½•trialå†å²
            self.trial_history.append({
                'trial_number': trial.number,
                'params': params,
                'cv_score': scores.mean(),
                'cv_std': scores.std(),
                'time_seconds': elapsed_time
            })

            return scores.mean()

        start_time = time.time()
        study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs)
        elapsed_time = time.time() - start_time

        # æ”¶æ•›æ›²çº¿
        best_scores = []
        current_best = -np.inf
        for trial in self.trial_history:
            if trial['cv_score'] > current_best:
                current_best = trial['cv_score']
            best_scores.append(current_best)

        results = {
            'method': 'optuna_tpe',
            'n_trials': n_trials,
            'total_time_seconds': elapsed_time,
            'best_params': study.best_params,
            'best_cv_score': study.best_value,
            'trial_history': self.trial_history,
            'convergence_curve': {
                'trial_index': list(range(n_trials)),
                'best_score_so_far': best_scores
            }
        }

        return results
```

### 6. è¯„ä¼°æŒ‡æ ‡

#### å¤šåˆ†ç±»LogLoss
```python
# evaluation/metrics.py
def compute_multiclass_logloss(y_true, y_pred_proba):
    """
    è®¡ç®—å¤šåˆ†ç±»logloss

    Args:
        y_true: çœŸå®æ ‡ç­¾ (n_samples,)
        y_pred_proba: é¢„æµ‹æ¦‚ç‡ (n_samples, n_classes)

    Returns:
        float: loglosså€¼
    """
    return log_loss(y_true, y_pred_proba, labels=list(range(7)))

def evaluate_cv_results(y_true, y_pred_proba_list, cv_folds):
    """
    è¯„ä¼°CVç»“æœ
    """
    scores = []
    for fold in range(len(cv_folds)):
        y_true_fold = [y_true[i] for i in cv_folds[fold]]
        y_pred_fold = y_pred_proba_list[fold]
        score = compute_multiclass_logloss(y_true_fold, y_pred_fold)
        scores.append(score)

    return {
        'mean_logloss': np.mean(scores),
        'std_logloss': np.std(scores),
        'fold_scores': scores
    }
```

## ğŸ“ å®éªŒç»“æœæ ¼å¼è§„èŒƒ

### JSONç»“æœæ ¼å¼
```json
{
  "experiment_metadata": {
    "experiment_id": "group_b_optuna_tpe",
    "experiment_group": "B",
    "timestamp": "2025-12-04T10:30:00",
    "python_version": "3.12.0",
    "libraries": {
      "scikit_learn": "1.3.0",
      "optuna": "3.5.0",
      "lightgbm": "4.1.0",
      "sentence_transformers": "2.2.2"
    }
  },
  "dataset_info": {
    "train_size": 23527,
    "n_classes": 7,
    "class_distribution": {
      "0": 3361,
      "1": 3361,
      "2": 3361,
      "3": 3361,
      "4": 3361,
      "5": 3361,
      "6": 3361
    },
    "split_strategy": "GroupKFold(n_splits=5)",
    "holdout_size": 4705
  },
  "experiment_config": {
    "feature_type": "tfidf_word_char",
    "model_type": "logistic_regression",
    "hpo_method": "optuna_tpe",
    "n_trials": 40,
    "cv_strategy": "GroupKFold(n_splits=5)",
    "scoring": "neg_log_loss"
  },
  "search_space": {
    "tfidf__word_ngram": {"type": "categorical", "choices": [[1,1], [1,2], [1,3]]},
    "tfidf__char_ngram": {"type": "categorical", "choices": [[3,5], [3,6]]},
    "tfidf__max_features": {"type": "int", "low": 20000, "high": 60000, "step": 20000},
    "classifier__C": {"type": "float", "low": 0.001, "high": 100, "log": true},
    "classifier__class_weight": {"type": "categorical", "choices": [null, "balanced"]}
  },
  "results": {
    "best_params": {
      "tfidf__word_ngram": [1, 2],
      "tfidf__char_ngram": [3, 5],
      "tfidf__max_features": 40000,
      "classifier__C": 0.854,
      "classifier__class_weight": "balanced"
    },
    "best_cv_score": -1.2345,
    "cv_scores_mean": -1.2456,
    "cv_scores_std": 0.0321,
    "cv_scores_fold": [-1.251, -1.238, -1.242, -1.249, -1.248],
    "holdout_score": -1.2298,
    "total_time_seconds": 3240.5,
    "per_trial_time_seconds": 81.0
  },
  "trial_history": [
    {
      "trial_id": 0,
      "params": {...},
      "cv_score": -1.5123,
      "cv_std": 0.0456,
      "time_seconds": 78.2
    },
    ...
  ],
  "convergence_curve": {
    "trial_index": [0, 1, 2, ..., 39],
    "best_score_so_far": [-1.512, -1.485, -1.463, ..., -1.234]
  },
  "resource_usage": {
    "peak_memory_mb": 2048,
    "avg_cpu_usage": 0.75
  }
}
```

### CSVå¯¹æ¯”è¡¨æ ¼æ ¼å¼
```csv
experiment_group,method,feature_type,model_type,n_trials,best_cv_score,best_cv_std,holdout_score,total_time_seconds,best_params_json,notes
A,baseline,tfidf_word,lr,0,-1.567,0.045,-1.543,120.5,"{C:1.0}","No HPO"
B,grid_search,tfidf_word_char,lr,32,-1.289,0.038,-1.265,2950.2,"{C:1.0, class_weight:balanced}",Exhaustive search
B,random_search,tfidf_word_char,lr,30,-1.276,0.035,-1.258,2234.1,"{C:0.854, class_weight:balanced}",Random sampling
B,optuna_tpe,tfidf_word_char,lr,40,-1.269,0.033,-1.251,2598.7,"{C:0.912, class_weight:balanced}",Bayesian optimization
C,grid_search,embedding,lgb,54,-1.198,0.029,-1.182,8520.3,"{num_leaves:63, lr:0.1}",Very slow
C,random_search,embedding,lgb,30,-1.187,0.031,-1.171,4750.8,"{num_leaves:95, lr:0.12}",Better efficiency
C,optuna_tpe,embedding,lgb,60,-1.175,0.028,-1.165,5680.4,"{num_leaves:127, lr:0.15}",Best performance
D,optuna_tpe,tfidf_word,lr,50,-1.298,0.036,-1.282,1650.0,"{C:1.2}","Word only"
D,optuna_tpe,tfidf_word_char,lr,50,-1.269,0.033,-1.251,2598.7,"{C:0.912}","Word+char"
D,optuna_tpe,embedding,lr,50,-1.256,0.034,-1.245,1820.3,"{C:0.785}","Embedding+LR"
D,optuna_tpe,embedding,lgb,50,-1.175,0.028,-1.165,5680.4,"{num_leaves:127}","Embedding+LGB"
```

## ğŸ“‹ å®ç°æ­¥éª¤

### 1. é¡¹ç›®åŸºç¡€æ­å»º
**ç›®æ ‡**: å»ºç«‹é¡¹ç›®éª¨æ¶å’ŒåŸºç¡€ä¾èµ–

**ä»»åŠ¡åˆ—è¡¨**:
1. **åˆ›å»ºç›®å½•ç»“æ„**
   ```bash
   mkdir -p src/{config,data,features,models,hpo,evaluation,experiments,utils,pipeline}
   mkdir -p results/{experiments/{group_a,group_b,group_c,group_d,group_e},cache/embeddings,final}
   mkdir -p logs
   mkdir -p scripts
   ```

2. **æ›´æ–°ä¾èµ–åŒ…** (`pyproject.toml`)
   ```toml
   [project]
   name = "predict-the-llm"
   version = "0.1.0"
   requires-python = "==3.12"
   dependencies = [
       "numpy>=1.24.0",
       "pandas>=2.0.0",
       "scikit-learn>=1.3.0",
       "lightgbm>=4.1.0",
       "optuna>=3.5.0",
       "sentence-transformers>=2.2.2",
       "matplotlib>=3.7.0",
       "seaborn>=0.12.0",
       "tqdm>=4.65.0",
       "joblib>=1.3.0"
   ]
   ```
   å®‰è£…ä¾èµ–: `pip install -e .`

3. **åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶**
   - `config/data_config.py`: æ•°æ®è·¯å¾„ã€åˆ’åˆ†ç­–ç•¥
   - `config/model_config.py`: è¶…å‚æ•°ç©ºé—´å®šä¹‰
   - `config/experiment_config.py`: 5ä¸ªå®éªŒç»„é…ç½®

4. **å®ç°åŸºç¡€å·¥å…·**
   - `utils/logger.py`: ç»Ÿä¸€æ—¥å¿—æ ¼å¼
   - `utils/io_utils.py`: JSON/CSVä¿å­˜åŠ è½½

**è¾“å‡º**:
- å®Œæ•´çš„ç›®å½•ç»“æ„
- é…ç½®æ–‡ä»¶æ¨¡æ¿
- æ—¥å¿—ç³»ç»Ÿå¯ç”¨

### 2. æ•°æ®å¤„ç†æ¨¡å—
**ç›®æ ‡**: å®ç°æ•°æ®åŠ è½½ã€åˆ’åˆ†å’Œé¢„å¤„ç†

**ä»»åŠ¡åˆ—è¡¨**:
1. **æ•°æ®åŠ è½½å™¨** (`data/data_loader.py`)
   ```python
   def load_data(train_path, test_path):
       train_df = pd.read_csv(train_path)
       test_df = pd.read_csv(test_path)
       return train_df, test_df

   def basic_preprocessing(df):
       # å»é™¤ç¼ºå¤±å€¼
       # åŸºç¡€æ–‡æœ¬æ¸…æ´—
       return df
   ```

2. **GroupKFoldåˆ’åˆ†å™¨** (`data/splitter.py`)
   - åŸºäºquestion hashåˆ›å»ºgroup
   - 80/20 train/holdout split
   - 5æŠ˜GroupKFold CV
   - ä¿æŒlabelåˆ†å¸ƒå¹³è¡¡

3. **æ–‡æœ¬é¢„å¤„ç†å™¨** (`data/preprocessor.py`)
   - æ‹¼æ¥questionå’Œanswer
   - å¯é€‰lowercase
   - å»é™¤å¤šä½™ç©ºç™½

4. **é›†æˆæµ‹è¯•**
   - åŠ è½½æ•°æ®å¹¶æ£€æŸ¥åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
   - éªŒè¯åˆ’åˆ†æ˜¯å¦ä¿è¯åŒquestionä¸è·¨è¶Šfold
   - ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®åˆ°cache

**è¾“å‡º**:
- æ•°æ®åŠ è½½ä¸åˆ’åˆ†å‡½æ•°
- é¢„å¤„ç†å™¨ç±»
- æ•°æ®å®Œæ•´æ€§éªŒè¯è„šæœ¬

### 3. ç‰¹å¾å·¥ç¨‹æ¨¡å—
**ç›®æ ‡**: å®ç°ä¸‰ç§ç‰¹å¾æå–å™¨

**ä»»åŠ¡åˆ—è¡¨**:
1. **TF-IDFç‰¹å¾æå–å™¨** (`features/tfidf_features.py`)
   - word-level TF-IDF
   - char-level TF-IDF
   - FeatureUnionç»„åˆä¸¤è€…
   - ç®¡é“åŒ–å¤„ç†(preprocessor + tfidf + classifier)

2. **å¥å‘é‡ç‰¹å¾æå–å™¨** (`features/embedding_features.py`)
   - é›†æˆsentence-transformers
   - all-MiniLM-L6-v2æ¨¡å‹
   - é¢„è®¡ç®—å¹¶ç¼“å­˜åˆ°.npy
   - æ”¯æŒå¢é‡è®¡ç®—(å·²æœ‰cacheåˆ™è·³è¿‡)

3. **ç»Ÿè®¡ç‰¹å¾æå–å™¨** (`features/statistical_features.py`)
   - æ–‡æœ¬é•¿åº¦(å­—ç¬¦ã€è¯æ•°)
   - æ ‡ç‚¹æ¯”ä¾‹ã€å¤§å†™æ¯”ä¾‹ã€æ•°å­—æ¯”ä¾‹
   - å¹³å‡è¯é•¿ã€å¥å­æ•°
   - å¯è§†åŒ–ä¸åŒLLMçš„ç»Ÿè®¡ç‰¹å¾åˆ†å¸ƒ

4. **ç»Ÿä¸€ç‰¹å¾æ¥å£** (`features/base.py`)
   - `BaseFeatureExtractor`åŸºç±»
   - `fit_transform()`æ ‡å‡†æ¥å£
   - æ”¯æŒåºåˆ—åŒ–(pickle)

**è¾“å‡º**:
- ä¸‰ç§ç‰¹å¾æå–å™¨
- ç¼“å­˜æœºåˆ¶
- ç‰¹å¾æ•ˆæœå¯¹æ¯”è„šæœ¬

### 4. æ¨¡å‹æ¨¡å—
**ç›®æ ‡**: å®ç°æ¨¡å‹å°è£…å’ŒPipeline

**ä»»åŠ¡åˆ—è¡¨**:
1. **åŸºç¡€æ¨¡å‹ç±»** (`models/base_model.py`)
   - ç»Ÿä¸€æ¥å£(fit, predict, predict_proba)
   - LRå’ŒLightGBMçš„é€‚é…
   - å¤šåˆ†ç±»é…ç½®(7ç±»)

2. **PipelineåŸºç±»** (`pipeline/base_pipeline.py`)
   ```python
   class BasePipeline:
       def __init__(self, feature_extractor, model):
           self.feature_extractor = feature_extractor
           self.model = model

       def fit(self, X_text, y, cv_folds=None):
           # ç‰¹å¾æå–
           X_features = self.feature_extractor.fit_transform(X_text)
           # è®­ç»ƒæ¨¡å‹
           self.model.fit(X_features, y)
           return self

       def predict_proba(self, X_text):
           X_features = self.feature_extractor.transform(X_text)
           return self.model.predict_proba(X_features)
   ```

3. **TF-IDF Pipeline** (`pipeline/tfidf_pipeline.py`)
   - é›†æˆé¢„å¤„ç†å™¨ + TF-IDF + æ¨¡å‹
   - æ”¯æŒç½‘æ ¼æœç´¢å‚æ•°

4. **Embedding Pipeline** (`pipeline/embedding_pipeline.py`)
   - åŠ è½½é¢„è®¡ç®—embedding + æ¨¡å‹
   - è½»é‡çº§pipeline

5. **é›†æˆæµ‹è¯•**
   - ç”¨å°æ•°æ®é›†éªŒè¯pipelineæ­£ç¡®æ€§
   - æ£€æŸ¥predict_probaè¾“å‡ºå½¢çŠ¶(7ç±»)

**è¾“å‡º**:
- å®Œæ•´çš„æ¨¡å‹å’ŒPipelineç±»
- Pipelineæ­£ç¡®æ€§éªŒè¯è„šæœ¬

### 5. HPOæ¨¡å—
**ç›®æ ‡**: å®ç°ä¸‰ç§HPOæ–¹æ³•

**ä»»åŠ¡åˆ—è¡¨**:
1. **Grid Searchå®ç°** (`hpo/grid_search.py`)
   - å°è£…GridSearchCV
   - è‡ªåŠ¨ç”Ÿæˆå‚æ•°ç½‘æ ¼
   - è®°å½•æ‰€æœ‰trialç»“æœ

2. **Random Searchå®ç°** (`hpo/random_search.py`)
   - å°è£…RandomizedSearchCV
   - å‚æ•°åˆ†å¸ƒå®šä¹‰(loguniform, randintç­‰)
   - æ”¶æ•›æ›²çº¿è®°å½•

3. **Optuna TPEå®ç°** (`hpo/optuna_tpe.py`)
   - TPESampleré…ç½®(seed=42)
   - MedianPrunerå‰ªæ
   - åŠ¨æ€å‚æ•°é‡‡æ ·(æ ¹æ®trialé€‰æ‹©)
   - Trialå†å²è®°å½•
   - æ”¶æ•›æ›²çº¿å¯è§†åŒ–

4. **ç»Ÿä¸€HPOæ¥å£**
   ```python
   class HPOManager:
       def __init__(self, method, pipeline, param_space, cv_strategy):
           if method == 'grid':
               self.hpo = GridSearchHPO(pipeline, param_space, cv_strategy)
           elif method == 'random':
               self.hpo = RandomSearchHPO(pipeline, param_space, cv_strategy)
           elif method == 'optuna':
               self.hpo = OptunaTPEHPO(pipeline, param_space, cv_strategy)

       def optimize(self, budget):
           return self.hpo.optimize(budget)
   ```

5. **è¯„ä¼°æŒ‡æ ‡é›†æˆ** (`evaluation/metrics.py`)
   - å¤šåˆ†ç±»loglossè®¡ç®—
   - CVç»“æœæ±‡æ€»
   - è¾…åŠ©æŒ‡æ ‡(Accuracy, F1)

**è¾“å‡º**:
- ä¸‰ç§HPOæ–¹æ³•
- ç»Ÿä¸€è°ƒç”¨æ¥å£
- ç»“æœè®°å½•å’Œå¯è§†åŒ–å·¥å…·

### 6. å®éªŒç»„å®ç°
**ç›®æ ‡**: å®ç°5ä¸ªå®éªŒç»„çš„å®Œæ•´é€»è¾‘

**ä»»åŠ¡åˆ—è¡¨**:
1. **å®éªŒç»„A** (`experiments/group_a_baseline.py`)
   - A1: åŸºç¡€TF-IDF+LR
   - A2: å¢å¼ºTF-IDF+LR
   - GroupKFold vs StratifiedKFoldå¯¹æ¯”(å¯é€‰)
   - è¾“å‡ºåŸºçº¿æ€§èƒ½

2. **å®éªŒç»„B** (`experiments/group_b_tfidf_lr.py`)
   - å›ºå®šTF-IDF+LR
   - ä¸‰ç§HPOæ–¹æ³•(B1-B3)
   - å‚æ•°ç½‘æ ¼/åˆ†å¸ƒå®šä¹‰
   - æ”¶æ•›æ›²çº¿å¯¹æ¯”

3. **å®éªŒç»„C** (`experiments/group_c_embedding_lgb.py`)
   - å›ºå®šSentence Embedding+LightGBM
   - é¢„è®¡ç®—embeddingç¼“å­˜
   - ä¸‰ç§HPOæ–¹æ³•å¯¹æ¯”
   - è®¡ç®—èµ„æºç›‘æ§

4. **å®éªŒç»„D** (`experiments/group_d_feature_comparison.py`)
   - D1-D4å››ç§ç»„åˆ
   - ç»Ÿä¸€Optuna TPE (n_trials=50)
   - æ€§èƒ½ã€æ•ˆç‡ã€ç»´åº¦å¯¹æ¯”
   - é›·è¾¾å›¾å¯è§†åŒ–

5. **å®éªŒç»„E** (`experiments/group_e_ablation.py`)
   - E1-E3: Q vs Aé‡è¦æ€§
   - E4: ç»Ÿè®¡ç‰¹å¾+LightGBM
   - E5: ç»Ÿè®¡ç‰¹å¾+TF-IDF
   - E6: é”™è¯¯åˆ†æ(æ··æ·†çŸ©é˜µ)

**è¾“å‡º**:
- 5ä¸ªå®éªŒç»„æ‰§è¡Œè„šæœ¬
- æ¯ä¸ªå®éªŒç»„çš„é…ç½®å’Œç»“æœæ ¼å¼

### 7. å®éªŒæ‰§è¡Œä¸ç»“æœæ”¶é›†
**ç›®æ ‡**: è¿è¡Œæ‰€æœ‰å®éªŒå¹¶æ”¶é›†ç»“æœ

**ä»»åŠ¡åˆ—è¡¨**:
1. **æ‰¹é‡æ‰§è¡Œè„šæœ¬** (`scripts/run_all_experiments.py`)
   ```python
   experiments = [
       ('A', 'baseline', None),
       ('B', 'grid', 32),
       ('B', 'random', 30),
       ('B', 'optuna', 40),
       ('C', 'grid', 54),
       ('C', 'random', 30),
       ('C', 'optuna', 60),
       ('D', 'optuna', 50),
       ('E', 'optuna', 30)
   ]

   for exp_group, method, budget in experiments:
       results = run_experiment(exp_group, method, budget)
       save_results(results)
   ```

2. **ç»“æœæ±‡æ€»**
   - è‡ªåŠ¨ç”Ÿæˆresults/experiment_summary.csv
   - åˆå¹¶æ‰€æœ‰å®éªŒçš„JSONç»“æœ
   - è¯†åˆ«æœ€ä½³é…ç½®

3. **å¯è§†åŒ–è„šæœ¬** (`utils/visualization.py`)
   - HPOæ”¶æ•›æ›²çº¿å¯¹æ¯”
   - ä¸åŒHPOæ–¹æ³•çš„æ¡å½¢å›¾
   - ç‰¹å¾ç±»å‹å¯¹æ¯”é›·è¾¾å›¾
   - æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾

4. **æ€§èƒ½ç›‘æ§**
   - è®°å½•æ¯å®éªŒç»„çš„è€—æ—¶
   - å†…å­˜ä½¿ç”¨ç›‘æ§
   - ç”Ÿæˆèµ„æºä½¿ç”¨æŠ¥å‘Š

**è¾“å‡º**:
- æ‰€æœ‰å®éªŒçš„å®Œæ•´ç»“æœ
- å¯¹æ¯”å›¾è¡¨å’ŒæŠ¥å‘Š
- èµ„æºä½¿ç”¨åˆ†æ

### 8. æœ€ç»ˆæ¨¡å‹ä¸æäº¤
**ç›®æ ‡**: è®­ç»ƒæœ€ä½³æ¨¡å‹å¹¶ç”ŸæˆKaggleæäº¤

**ä»»åŠ¡åˆ—è¡¨**:
1. **é€‰æ‹©æœ€ä½³é…ç½®**
   ```python
   # ä»å®éªŒç»“æœä¸­é€‰æ‹©
   best_exp = find_best_experiment(results_summary_csv)
   print(f"Best config: {best_exp}")
   ```

2. **å…¨é‡æ•°æ®è®­ç»ƒ**
   ```python
   # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®(ä¸å†åˆ’åˆ†holdout)
   final_pipeline = build_pipeline(best_exp.config)
   final_pipeline.fit(train_df['text'], train_df['target'])
   ```

3. **é¢„æµ‹æµ‹è¯•é›†**
   ```python
   test_probs = final_pipeline.predict_proba(test_df['text'])
   submission = pd.DataFrame({
       'id': test_df['id'],
       'target_0': test_probs[:, 0],
       'target_1': test_probs[:, 1],
       ...
       'target_6': test_probs[:, 6]
   })
   submission.to_csv('results/final/submission.csv', index=False)
   ```

4. **é”™è¯¯åˆ†æ**
   - æ··æ·†çŸ©é˜µåˆ†æ
   - è¯†åˆ«æœ€å®¹æ˜“æ··æ·†çš„LLMå¯¹
   - é€‰æ‹©3-5ä¸ªå…¸å‹é”™è¯¯æ¡ˆä¾‹
   - åˆ†æä¸ºä»€ä¹ˆé”™è¯¯

5. **æ’°å†™æ€»ç»“æŠ¥å‘Š**
   ```markdown
   ## å®éªŒç»“è®º

   ### Q1: HPOæ–¹æ³•å¯¹æ¯”
   - Optuna TPEåœ¨æ‰€æœ‰åœºæ™¯ä¸‹å‡ä¼˜äºGridå’ŒRandom Search
   - åœ¨é«˜ç»´ç©ºé—´(TPE+LightGBM)ï¼Œä¼˜åŠ¿æ›´æ˜æ˜¾
   - æ”¶æ•›é€Ÿåº¦: TPE > Random > Grid

   ### Q2: ç‰¹å¾ä¸æ¨¡å‹
   - Sentence Embedding + LightGBM æ•ˆæœæœ€ä½³
   - TF-IDF+LR æ€§ä»·æ¯”æœ€é«˜(è®­ç»ƒå¿«+æ˜“å®ç°+æ•ˆæœå¥½)
   - ç»Ÿè®¡ç‰¹å¾æœ‰ä¸€å®šå¸®åŠ©ä½†æå‡æœ‰é™

   ### Q3: æ¨èé…ç½®
   - èµ„æºå……è¶³: Sentence Embedding + LightGBM + Optuna TPE
   - èµ„æºæœ‰é™: TF-IDF(word+char) + LR + Optuna TPE
   ```

**è¾“å‡º**:
- final/submission.csv
- é”™è¯¯åˆ†ææŠ¥å‘Š
- å®éªŒæ€»ç»“æŠ¥å‘Š
- æœ€ä½³æ¨¡å‹.pklæ–‡ä»¶

## ğŸ” å…³é”®å®ç°ç»†èŠ‚

### 1. ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
```python
# utils/logger.py
import logging
from datetime import datetime

def setup_logger(name, log_file, level=logging.INFO):
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler = logging.FileHandler(log_file)
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger

# ä½¿ç”¨ç¤ºä¾‹
logger = setup_logger('experiment', f'logs/experiment_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
logger.info("Starting experiment group B...")
```

### 2. ç»“æœä¿å­˜ä¸åŠ è½½
```python
# utils/io_utils.py
import json
import pandas as pd
from datetime import datetime

def save_experiment_results(results, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_file = os.path.join(output_dir, f"{timestamp}_results.json")

    with open(json_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print(f"Results saved to {json_file}")
    return json_file

def load_experiment_results(json_file):
    with open(json_file, 'r') as f:
        return json.load(f)

def create_summary_csv(results_dir, output_file):
    all_results = []
    for json_file in glob(os.path.join(results_dir, "*.json")):
        r = load_experiment_results(json_file)
        all_results.append({
            'experiment_group': r['experiment_config']['experiment_group'],
            'method': r['experiment_config']['hpo_method'],
            'best_cv_score': r['results']['best_cv_score'],
            'holdout_score': r['results']['holdout_score'],
            'total_time_seconds': r['results']['total_time_seconds']
        })

    df = pd.DataFrame(all_results)
    df.to_csv(output_file, index=False)
    print(f"Summary CSV saved to {output_file}")
```

### 3. å¹¶è¡ŒåŒ–åŠ é€Ÿ
```python
# Grid Searchå’ŒRandom Search
search = GridSearchCV(..., n_jobs=-1)  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ

# Optuna
study.optimize(objective, n_jobs=4)  # 4ä¸ªå¹¶è¡Œtrial

# äº¤å‰éªŒè¯
scores = cross_val_score(..., cv=5, n_jobs=-1)
```

### 4. å‰ªæç­–ç•¥
```python
# Optuna MedianPruner
pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)
# 5æŠ˜CVä¸­ï¼Œå¦‚æœå½“å‰trialçš„åˆ†æ•°ä½äºå‰5æŠ˜çš„ä¸­ä½æ•°ï¼Œåˆ™æå‰åœæ­¢

# LightGBMæ—©åœ
lgb.LGBMClassifier(
    n_estimators=1000,
    early_stopping_rounds=50,
    eval_set=[(X_val, y_val)],
    verbose=-1
)
```

### 5. å†…å­˜ä¼˜åŒ–
```python
# åˆ†æ‰¹å¤„ç†å¤§æ–‡æœ¬
def batch_encode(texts, batch_size=64):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch)
        embeddings.append(batch_emb)
    return np.vstack(embeddings)

# ç¨€ç–çŸ©é˜µå­˜å‚¨
from scipy.sparse import csr_matrix
tfidf_matrix = csr_matrix(tfidf_features)
```

## ğŸ“ˆ å¯è§†åŒ–æ–¹æ¡ˆ

### 1. HPOæ”¶æ•›æ›²çº¿
```python
def plot_convergence(results_list, save_path):
    plt.figure(figsize=(10, 6))
    for results in results_list:
        curve = results['convergence_curve']
        plt.plot(curve['trial_index'], curve['best_score_so_far'],
                label=f"{results['method']}")
    plt.xlabel('Trial Index')
    plt.ylabel('Best CV Score (higher is better)')
    plt.title('HPO Convergence Comparison')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path, dpi=300)
```

### 2. æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
```python
def plot_performance_comparison(summary_df, save_path):
    plt.figure(figsize=(12, 6))
    methods = summary_df['method'].unique()

    x = np.arange(len(methods))
    width = 0.35

    for i, group in enumerate(['B', 'C']):
        group_data = summary_df[summary_df['experiment_group'] == group]
        scores = [group_data[group_data['method'] == m]['best_cv_score'].values[0]
                 for m in methods if m in group_data['method'].values]
        plt.bar(x[:len(scores)] + i*width, scores, width,
               label=f'Group {group}', alpha=0.8)

    plt.xlabel('HPO Method')
    plt.ylabel('Best CV Score')
    plt.title('HPO Performance Comparison')
    plt.xticks(x + width/2, methods)
    plt.legend()
    plt.grid(True, axis='y')
    plt.savefig(save_path, dpi=300)
```

### 3. ç‰¹å¾ç±»å‹é›·è¾¾å›¾
```python
def plot_feature_radar(summary_df, save_path):
    categories = ['Performance', 'Training Speed', 'Memory Efficiency', 'Implementation Complexity']
    D1 = [0.65, 0.95, 0.90, 0.95]  # TF-IDF+LR
    D2 = [0.72, 0.90, 0.85, 0.90]  # TF-IDF+char+LR
    D3 = [0.75, 0.70, 0.60, 0.75]  # Embedding+LR
    D4 = [0.88, 0.50, 0.55, 0.60]  # Embedding+LightGBM

    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ

    D1 += D1[:1]
    D2 += D2[:1]
    D3 += D3[:1]
    D4 += D4[:1]

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
    ax.plot(angles, D1, 'o-', linewidth=2, label='D1: TF-IDF+LR')
    ax.fill(angles, D1, alpha=0.25)
    ax.plot(angles, D2, 'o-', linewidth=2, label='D2: TF-IDF+char+LR')
    ax.fill(angles, D2, alpha=0.25)
    ax.plot(angles, D3, 'o-', linewidth=2, label='D3: Embedding+LR')
    ax.fill(angles, D3, alpha=0.25)
    ax.plot(angles, D4, 'o-', linewidth=2, label='D4: Embedding+LightGBM')
    ax.fill(angles, D4, alpha=0.25)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1)
    ax.set_title('Feature+Model Combination Comparison', y=1.08)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    plt.savefig(save_path, dpi=300)
```

### 4. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
```python
def plot_confusion_matrix(y_true, y_pred, class_names, save_path):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted LLM')
    plt.ylabel('True LLM')
    plt.title('Confusion Matrix - LLM Identification')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
```

## âš ï¸ æ½œåœ¨é£é™©ä¸è§£å†³æ–¹æ¡ˆ

### 1. å†…å­˜ä¸è¶³
**é£é™©**: Embeddingé¢„è®¡ç®—å ç”¨å¤§é‡å†…å­˜(23KÃ—384â‰ˆ9MBï¼Œå¯æ¥å—)

**è§£å†³æ–¹æ¡ˆ**:
- åˆ†æ‰¹è®¡ç®—(æ¯æ‰¹64æ ·æœ¬)
- ä½¿ç”¨`np.save()`å‹ç¼©å­˜å‚¨
- åŠæ—¶é‡Šæ”¾ä¸­é—´å˜é‡

### 2. è®­ç»ƒæ—¶é—´è¿‡é•¿
**é£é™©**: LightGBMåœ¨Large SearchSpaceä¸‹è®­ç»ƒæ…¢

**è§£å†³æ–¹æ¡ˆ**:
- è®¾ç½®`early_stopping_rounds=50`
- ä½¿ç”¨`Optuna MedianPruner`å‰ªæ
- é™åˆ¶`n_trials`åœ¨åˆç†èŒƒå›´(30-60)

### 3. æ•°æ®æ³„éœ²
**é£é™©**: åŒquestionåœ¨trainå’Œvalä¸­åŒæ—¶å‡ºç°

**è§£å†³æ–¹æ¡ˆ**:
- å¿…é¡»ä½¿ç”¨`GroupKFold`
- åŸºäºquestion textçš„hashåˆ’åˆ†
- åœ¨Aç»„å®éªŒä¸­å¯¹æ¯”éªŒè¯

### 4. ç»“æœä¸ç¨³å®š
**é£é™©**: éšæœºç§å­ä¸åŒå¯¼è‡´ç»“æœå·®å¼‚

**è§£å†³æ–¹æ¡ˆ**:
- è®¾ç½®å›ºå®šrandom_state
- å¤šæ¬¡è¿è¡Œå–å¹³å‡
- è®°å½•ä¸åŒseedä¸‹çš„æ–¹å·®

### 5. æäº¤æ ¼å¼é”™è¯¯
**é£é™©**: Kaggleè¦æ±‚ä¸¥æ ¼æ ¼å¼

**è§£å†³æ–¹æ¡ˆ**:
```python
# éªŒè¯æäº¤æ ¼å¼
def validate_submission(submission_df):
    assert submission_df.shape[0] == 6008
    assert list(submission_df.columns) == ['id', 'target_0', ..., 'target_6']
    assert np.allclose(submission_df.iloc[:, 1:].sum(axis=1), 1.0)
    print("Submission format validated!")
```

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

### 1. å®éªŒå®Œæ•´æ€§
- âœ… æ‰€æœ‰5ä¸ªå®éªŒç»„å‡æˆåŠŸè¿è¡Œ
- âœ… æ¯ä¸ªå®éªŒç»„çš„ç»“æœæ–‡ä»¶å®Œæ•´(JSON+CSV)
- âœ… æ— æ•°æ®æ³„éœ²æˆ–æ ¼å¼é”™è¯¯

### 2. æ€§èƒ½æŒ‡æ ‡
- **ç»„Bæœ€ä½³CV logloss**: < 1.28
- **ç»„Cæœ€ä½³CV logloss**: < 1.20
- **æœ€ç»ˆKaggle logloss**: < 1.18 (ç›®æ ‡)

### 3. æ•ˆç‡æŒ‡æ ‡
- **ç»„Bä¸‰æ–¹æ³•å¯¹æ¯”**: Optuna TPEæ”¶æ•›æœ€å¿«
- **ç»„Cä¸‰æ–¹æ³•å¯¹æ¯”**: TPEåœ¨å¤æ‚ç©ºé—´ä¼˜åŠ¿æ˜æ˜¾
- **æ€»ä½“æ—¶é—´é¢„ç®—**: < 6å°æ—¶(CPUè®­ç»ƒ)

### 4. ä»£ç è´¨é‡
- âœ… æ¨¡å—åŒ–è®¾è®¡ï¼Œå¯å¤ç”¨
- âœ… ç»Ÿä¸€æ¥å£ï¼Œæ˜“æ‰©å±•
- âœ… å®Œæ•´æ³¨é‡Šå’Œæ–‡æ¡£
- âœ… ç»“æœæ ¼å¼ç»Ÿä¸€ï¼Œä¾¿äºæ±‡æ€»

## ğŸ“š æ‰©å±•æ–¹å‘(å¯é€‰)

### 1. é«˜çº§ç‰¹å¾
- **N-gram TF-IDF**: å°è¯•(1,4), (1,5)
- **é¢„è®­ç»ƒè¯å‘é‡**: Word2Vec, GloVe
- **BERT Embedding**: ä½†è®¡ç®—æˆæœ¬é«˜

### 2. é«˜çº§æ¨¡å‹
- **XGBoost**: ä¸LightGBMå¯¹æ¯”
- **ç¥ç»ç½‘ç»œ**: è½»é‡MLP
- **Ensemble**: æ¨¡å‹èåˆæå‡

### 3. é«˜çº§HPO
- **Optuna Samplerå¯¹æ¯”**: TPESampler vs CmaEsSampler vs NSGAIISampler
- **Prunerå¯¹æ¯”**: MedianPruner vs SuccessiveHalvingPruner
- **å¤šç›®æ ‡ä¼˜åŒ–**: åŒæ—¶ä¼˜åŒ–loglosså’Œè®­ç»ƒæ—¶é—´

### 4. æ·±åº¦åˆ†æ
- **SHAP**: è§£é‡Šæ¨¡å‹é¢„æµ‹
- **LIME**: å•æ ·æœ¬è§£é‡Š
- **Attentionå¯è§†åŒ–**: å¦‚æœä½¿ç”¨Transformer

---

## ğŸ“ æ€»ç»“

æœ¬æ–¹æ¡ˆæä¾›äº†å®Œæ•´çš„HPOå®éªŒå®ç°è“å›¾ï¼Œæ¶µç›–ï¼š

1. **ç³»ç»ŸåŒ–å®éªŒè®¾è®¡**: 5ä¸ªå®éªŒç»„å¾ªåºæ¸è¿›
2. **æ¨¡å—åŒ–ä»£ç æ¶æ„**: é«˜å†…èšä½è€¦åˆï¼Œæ˜“ç»´æŠ¤
3. **ç»Ÿä¸€ç»“æœæ ¼å¼**: JSON+CSVï¼Œä¾¿äºå¯¹æ¯”åˆ†æ
4. **è¯¦ç»†å®ç°æ­¥éª¤**: å¯æ‰§è¡Œçš„å¼€å‘æŒ‡å—
5. **é£é™©æ§åˆ¶æœºåˆ¶**: è¯†åˆ«é—®é¢˜å¹¶æä¾›è§£å†³æ–¹æ¡ˆ

é€šè¿‡éµå¾ªæœ¬æ–¹æ¡ˆï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç³»ç»Ÿå¯¹æ¯”ä¸åŒHPOæ–¹æ³•çš„æ•ˆæœ
- æ·±å…¥ç†è§£ç‰¹å¾å·¥ç¨‹çš„å½±å“
- æ‰¾å‡ºæœ€é€‚åˆèµ„æºçº¦æŸçš„pipeline
- ç”Ÿæˆé«˜è´¨é‡çš„å®éªŒæŠ¥å‘Š

**ä¸‹ä¸€æ­¥**: ç¡®è®¤è®¡åˆ’åå¼€å§‹å®æ–½ï¼
